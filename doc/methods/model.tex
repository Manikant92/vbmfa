\documentclass[a4paper]{article}
\usepackage[
    bookmarks,
    bookmarksopen=true,
    backref,
    plainpages=false,
    pdfpagelabels,
    hypertexnames=false,
    linktocpage,
    colorlinks=true,
    linkcolor=blue,
    anchorcolor=blue,
    citecolor=blue,
    filecolor=blue,
    menucolor=blue,
    urlcolor=cyan, 
]{hyperref}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{booktabs}
\usepackage{verbatim}
\usepackage{hyperref}
\usepackage[parfill]{parskip}

\author{Angermueller Christof}
\date{\today}
\title{Variational Bayesian Mixture of Factor Analysers}

\begin{document}
\maketitle

\newcommand{\bs}{\boldsymbol}
\newcommand{\Xp}{\bs{\pi}}

\newcommand{\Xn}{\bs{\nu}^s}
\newcommand{\Xnq}{\nu^s_q}

\newcommand{\Xl}{\Lambda^s}
\newcommand{\Xlp}{\bs{\Lambda}^s_p}
\newcommand{\Xlpq}{\Lambda^s_{pq}}
\newcommand{\Xlm}{\overline{\Lambda}^s}
\newcommand{\Xlmp}{\bar{\bs{\Lambda}}^s_p}
\newcommand{\Xlvp}{\Sigma^{p,s}}
\newcommand{\Xlvpi}{\Xlvp{}^{-1}}
\newcommand{\Xlvpll}{\Xlvp_{\Lambda\Lambda}}
\newcommand{\Xlvplu}{\Xlvp_{\Lambda\mu}}
\newcommand{\Xlvpul}{\Xlvp_{\mu\Lambda}}
\newcommand{\Xlvpuu}{\Xlvp_{\mu\mu}}
\newcommand{\Xlvplli}{\Xlvpll{}^{-1}}
\newcommand{\Xlvplui}{\Xlvplu{}^{-1}}
\newcommand{\Xlvpuli}{\Xlvpul{}^{-1}}
\newcommand{\Xlvpuui}{\Xlvpuu{}^{-1}}
\newcommand{\Xlt}{\tilde{\Lambda}^s}
\newcommand{\Xltb}{\tilde{{\bs{\Lambda}}}^s}
\newcommand{\Xltp}{\tilde{\bs{\Lambda}}^s_p}
\newcommand{\Xltpq}{\tilde{\Lambda}^s_{pq}}
\newcommand{\Xltm}{\bar{\tilde{\Lambda}}^s}
\newcommand{\Xltmp}{\bar{\tilde{\bs{\Lambda}}}^s_p}
\newcommand{\Xltmpq}{\bar{\tilde{\Lambda}}^s_{pq}}
\newcommand{\Xltvp}{\tilde{\Gamma}^{p,s}}
\newcommand{\Xltvpi}{\Xltvp{}^{-1}}

\newcommand{\Xu}{\bs{\mu}^s}
\newcommand{\Xup}{\mu^s_p}
\newcommand{\Xum}{\bar{\bs{\mu}}^s}
\newcommand{\Xump}{\bar{\mu}^s_p}

% Data
\newcommand{\Xy}{\bs{y}^n}
\newcommand{\Xs}{s^n}

\newcommand{\Xx}{\bs{x}^n}
\newcommand{\Xxm}{\overline{\bs{x}}^{n,s}}
\newcommand{\Xxv}{\Sigma^s}
\newcommand{\Xxt}{\tilde{\bs{x}}^n}
\newcommand{\Xxtq}{\tilde{x}^n_q}
\newcommand{\Xxtm}{\bar{\tilde{\bs{x}}}^n}
\newcommand{\Xxtmq}{\bar{\tilde{x}}^n_q}
\newcommand{\Xxtv}{\tilde{\Sigma}^s}

% Hyperparameters
\newcommand{\Xpsi}{\Psi}
\newcommand{\Xpsii}{\Psi^{-1}}
\newcommand{\Xha}{a^*}
\newcommand{\Xhb}{b^*}
\newcommand{\Xhal}{\alpha^*}
\newcommand{\Xhm}{\bs{m}^*}
\newcommand{\Xhms}{m^*_s}
\newcommand{\Xhu}{\bs{\mu}^*}
\newcommand{\Xhup}{\mu^*_p}
\newcommand{\Xhn}{\bs{\nu}^*}
\newcommand{\Xhnp}{\nu^*_p}

% Operators
\newcommand{\Xcov}{\operatorname{cov}}
\newcommand{\Xvar}{\operatorname{var}}
\newcommand{\Xtr}{\operatorname{tr}}
\newcommand{\Xdiag}{\operatorname{diag}}
\newcommand{\Xvec}{\operatorname{vec}}

% Distributions
\newcommand{\Xdn}{\mathcal{N}}
\newcommand{\Xdm}{\mathcal{M}}
\newcommand{\Xdg}{\operatorname{Ga}}
\newcommand{\Xdd}{\mathcal{D}}




\newcommand{\Xyg}{\bs{y}^{n,g}}
\newcommand{\Xxg}{\bs{x}^{n,g}}
\newcommand{\Xsg}{\bs{s}^{n,g}}

\begin{comment}
\section{Factor analysis}
\begin{table}[h]
  \begin{center}
  \begin{tabular}{lll}
    \toprule
    Symbol & Dimension & Description \\ \hline
    $\bs{y}$ & $P$ & Observed data vector \\
    $\bs{x}$ & $Q$ & Latent factors \\
    $\bs{\eta}$ & $P$ & Random Gaussian noise \\
    $\Lambda$ & $P \times Q$ & Factor loading matrix \\
    $\Lambda_{:q}$ & $P$ & Column loading matrix/principal component \\
    $\bs{\mu}$ & $P$ & Mean $\bs{y}$ \\
    $\bs{z}$ & $Q$ & Indicators factor analysers \\
    $\Xp$ & $S$ & Mixing proportions \\
    \bottomrule
  \end{tabular}
\end{center}
\end{table}
\begin{align}
  P(\bs{x})=\mathcal{N}(\bs{x}|0,I_Q)
\end{align}
\begin{align}
  P(\bs{\eta})=\mathcal{N}(\bs{\eta}|0,\Psi)
\end{align}

\subsection{Single factor analyser}
\begin{align}
  \bs{y}=\Lambda\bs{x}+\bs{\mu}+\bs{\eta}
\end{align}
\begin{align}
  P(\bs{y}|\bs{x},\Lambda,\bs{\mu},\bs{\eta})=\mathcal{N}(\bs{y}|\Lambda\bs{x}+\bs{\mu},\Psi)
\end{align}

\subsection{Mixture factor analysers}
\begin{align}
  \bs{y}=\bs{\eta}+\sum_{s=1}^S(\Lambda^s\bs{x}+\bs{\mu}) P(z_s|\bs{\pi})
\end{align}
\begin{align}
  P(\bs{y}|\bs{x},\Lambda,\bs{\mu},\bs{\eta})=\sum_{s=1}^S\mathcal{N}(\bs{y}|\Lambda\bs{x}+\bs{\mu},\Psi)P(z_s|\bs{\pi})
\end{align}
\newpage

\end{comment}

\section{Notations}
\begin{table}[h]
  \begin{center}
  \begin{tabular}{ll}
    \toprule
    Symbol & Description \\ \hline
    $P$ & Dimensionality data \\
    $Q$ & Number of factors/components \\
    $S$ & Number of factor analysers \\
    $N$ & Number of Individuals  \\
    $G$ & Number of genes \\
    \bottomrule
  \end{tabular}
  \caption{Indices.}
  \end{center}
\end{table}
\begin{table}[h]
  \begin{center}
  \begin{tabular}{lll}
    \toprule
    Symbol & Dimension & Description \\ \hline
    $\Xp$ & $S$ & Mixing proportions \\
    $\Xn$ & $Q$ & Precisions components factor analyser $s$ \\
    $\Xl$ & $P \times Q$ & Loading matrix factor analyser $s$ \\
    $\Xu$ & $P$ & Offset factor analyser $s$ \\
    $\Xyg$ & $P$ & Counts sample $n$ gene $g$ \\
    $\Xxg$ & $Q$ & Factors sample $n$ gene $g$ \\
    $\Xsg$ & $Q$ & Indicators factor analysers sample $n$ gene $g$ \\
    \bottomrule
  \end{tabular}
  \caption{Random variables.}
\end{center}
\end{table}
\begin{table}[h]
  \begin{center}
  \begin{tabular}{lll}
    \toprule
    Symbol & Dimension & Description \\ \hline
    $\Xha$ & $1$ & Shape gamma distribution \\
    $\Xhb$ & $1$ & Rate gamma distribution \\
    $\Xhal$ & $1$ & Strength Dirichlet prior \\
    $\Xhm$ & $S$ & Prior mixing proportions \\
    $\Xhu$ & $P$ & Offset mean \\
    $\Xhn$ & $P$ & Offset precision \\
    $\Xpsi$ & $P \times P$ & Noise matrix \\
    \bottomrule
  \end{tabular}
  \caption{Hyperparameters.}
\end{center}
\end{table}

\section{Model}
\begin{figure}[h]
  \begin{center}
    \begin{tikzpicture}
      \tikzstyle{sVar}=[circle, draw=black, very thick, minimum size=.8cm];
      \tikzstyle{sPlate}=[rectangle, dashed, draw=black, thick, align=flush right];
      \tikzstyle{sHyper}=[rectangle, draw=black, thick, minimum size=.8cm];
      \tikzstyle{sDep}=[->, thick];
      \node[sPlate, text width=5cm, text height=4.5cm] (Pn) {$n=1,...,N$};
      \node[sPlate, text width=4.5cm, text height=3.5cm, below=.5cm of Pn.north] (Pg) {$g=1,...,G$};
      \node[sVar, below left=.75cm and .5cm of Pg.north, fill=gray!50] (Y) {$\Xyg$};
      \node[sVar, right=1cm of Y] (Z) {$\Xsg$};
      \node[sVar, below=.5cm of Z] (X) {$\Xxg$};
      \node[sVar, right=1.5cm of Z] (Pi) {$\Xp$};
      \node[sHyper, left=2cm of Y] (Psi) {$\Xpsi$};
      \node[sHyper, right=1cm of Pi] (Alpha) {$\alpha^*\bs{m}^*$};
      \node[sPlate, align=flush right, text width=4cm, text depth=2.5cm, above=1.5cm of Y] (Ps) {$s=1,...,S$};
      \node[above =.75cm of Ps.south] (Sh) {};
      \node[sVar, left=.5cm of Sh] (L) {$\Lambda$};
      \node[sVar, above=.5cm of L] (Nu) {$\bs{\nu}$};
      \node[sVar, right=.5cm of Sh] (Mu) {$\bs{\mu}$};
      \node[sHyper, right=1cm of Mu] (MuH) {$\bs{\mu}^*,\bs{\nu}^*$};
      \node[sHyper, left=1cm of Nu] (NuH) {$a^*,b^*$};
      \draw[sDep] (Z) -- (Y);
      \draw[sDep] (X) -- (Y);
      \draw[sDep] (Psi) -- (Y);
      \draw[sDep] (Pi) -- (Z);
      \draw[sDep] (Alpha) -- (Pi);
      \draw[sDep] (L) -- (Y);
      \draw[sDep] (Mu) -- (Y);
      \draw[sDep] (Nu) -- (L);
      \draw[sDep] (NuH) -- (Nu);
      \draw[sDep] (MuH) -- (Mu);
    \end{tikzpicture}
    \caption{Graphical model}
  \end{center}
\end{figure}
\begin{align}
  P(\Xp,\bs{\nu},\Lambda,\bs{\mu},\bs{s},\bs{x},\bs{y})
  &=P(\Xp|\Xhal\Xhm)\prod_{s,q}P(\Xnq|\Xha,\Xhb)\prod_{s,p}P(\Xlp|\Xn)\prod_{s}P(\Xu|\Xhu,\Xhn) \\
  &\quad \prod_{n,g}P(\Xxg)\sum_sP(\Xsg|\Xp)P(\Xyg|\Xsg,\Xxg,\Xl,\Xu,\Xpsi)
\end{align}

\subsection{Conditional probability distributions}
\subsubsection{Parameters}
\begin{align}
  \Theta=\left\{\bs{\pi},\bs{\nu},\Lambda,\bs{\mu}\right\}
\end{align}
\begin{align}
  P(\Xp|\Xhal\Xhm)=\Xdd(\Xp|\Xhal\Xhm)
\end{align}
\begin{align}
  P(\bs{\nu}|\Xha,\Xhb)=\prod_{s,q}\Xdg(\Xnq|\Xha,\Xhb)
\end{align}
\begin{align}
  P(\Lambda|\Xn)=\prod_{s,p}\Xdn(\Xlp|0,\Xdiag[\Xn]^{-1})
\end{align}
\begin{align}
  P(\bs{\mu}|\Xhu,\Xhn)=\prod_s\Xdn(\Xu|\Xhu,\Xdiag[\Xhn]^{-1})
\end{align}
\subsubsection{Data}
\begin{align}
  D=\left\{\bs{y},\bs{x},\bs{s}\right\}
\end{align}
\begin{align}
  P(\Xx)=\Xdn(\Xx|0,I_{Q})
\end{align}
\begin{align}
  P(\Xs|\Xp)=\Xdm(\Xs|\Xp)
\end{align}
\begin{align}
  P(\Xy|\Xx,\Xs,\Lambda,\Xu,\Xpsi)=\Xdn(\Xy|\Xl\Xx+\Xu,\Xpsi)
\end{align}

\section{Variational approximation}
The goal is to approximate the following posterior distribution:
\begin{align}
  P(\bs{\pi},\bs{\nu},\Lambda,\bs{\mu},\bs{x},\bs{s}|\bs{y})=Q(\bs{\pi},\bs{\nu},\Lambda,\bs{\mu},\bs{x},\bs{s})
\end{align}
The following factorization is assumed:
\begin{align}
  Q(\bs{\pi},\bs{\nu},\Lambda,\bs{\mu},\bs{x},\bs{s})=Q(\bs{\pi},\bs{\nu})Q(\Lambda,\bs{\mu})Q(\bs{x},\bs{s})
\end{align}
Which further factorizes into:
\begin{align}
  Q(\bs{\pi},\bs{\nu})Q(\Lambda,\bs{\mu})Q(\bs{x},\bs{s})=Q(\bs{\pi})\prod_s\prod_q Q(\nu_q^s)\prod_s\prod_p Q(\tilde{\Lambda}^s_p) \prod_n Q(\Xx|\Xs)Q(\Xs)
\end{align}
Here, $\Xlt$ is the concatenation of $\Xl$ and $\bs{\mu}^s$:
\begin{align}
  \Xlt:=[\Xl \: \Xu].
\end{align}
The factors are distributed as follows:
\begin{align}
  Q(\Xp)=\mathcal{D}(\Xp|\alpha\bs{m})
\end{align}
\begin{align}
 Q(\nu^s_q)=\mathcal{G}(\nu^s_q|a, b_q^s)
\end{align}
\begin{align}
  Q(\Xltp)=\mathcal{N}(\Xltp|\Xltmp,\Xltvp)
\end{align}
\begin{align}
  Q(\Xx|\Xs)=\mathcal{N}(\Xx|\Xxm,\Xxv)
\end{align}
\begin{align}
  Q(\Xs)\text{ is }N\times S
\end{align}

\subsection{$Q(\Xp)$}
\begin{align}
  Q(\Xp)=\mathcal{D}(\Xp|\alpha\bs{m})
\end{align}
\begin{align}
  \alpha m_s=\Xhal\Xhms+\sum_n Q(\Xs)
\end{align}

\subsection{$Q(\Xnq)$}
\begin{align}
 Q(\Xnq)=\mathcal{G}(\Xnq|a, b_q^s)
\end{align}
\begin{align}
a&=a^*+\frac{P}{2} \\
b_q^s&=b^*+\frac{1}{2}\sum_p<\Xlpq{}^2> \\
&=b+\frac{1}{2}\sum_p\left(\Xltmpq{}^2 + \Xltvp_{qq}\right)
\end{align}
Here, we solved $<\Xltpq{}^2>$ by eq.~\ref{eq:x2}.

\subsection{$Q(\Xltp)$}
\begin{align}
  Q(\Xltp)=\mathcal{N}(\Xltp|\Xltmp,\Xltvp)
\end{align}
The extended loading matrix $\Xlt$ is $P\times (Q+1)$ and factorizes over rows:
\begin{align}
  \Xlt=\left[
    \begin{array}{ccc|c}
      \Xl_{11} & \cdots & \Xl_{1Q} & \mu^s_1 \\
      \vdots & \vdots & \vdots & \vdots \\
      \Xl_{p1} & \cdots & \Xl_{pQ} & \mu^s_p \\
      \vdots & \vdots & \vdots & \vdots \\
      \Xl_{P1} & \cdots & \Xl_{PQ} & \mu^s_P
    \end{array}
  \right]
\end{align}
The row mean is $(Q+1)\times 1$:
\begin{align}
  \Xltmp=\begin{bmatrix}
    \Xlmp \\
    \Xump
  \end{bmatrix}
\end{align}
\begin{align}
  \Xlmp
  &=\Xltvp_{\Lambda\Lambda}\left(\Xpsii_{pp}\sum_n Q(\Xs)y^n_p<\Xx>\right) \\
  &=\Xltvp_{\Lambda\Lambda}\left(\Xpsii_{pp}\sum_n Q(\Xs)y^n_p\Xxm\right)
\end{align}
\begin{align}
  \Xump
  &=\Xltvp_{\mu\mu}\left(\Xpsii_{pp}\sum_n Q(\Xs)y^n_p+\nu^*_p\mu^*_p\right)
\end{align}
The row covariance matrix is $P\times(Q+1)$ and decomposes into $4$ blocks:
\begin{align}
  \Xltvpi=\begin{bmatrix}
    \Xlvplli & \Xlvplui \\
    \Xlvpuli & \Xlvpuui
  \end{bmatrix}
\end{align}
$\Xlvpll$ is $Q\times Q$:
\begin{align}
  \Xlvplli
  &=\Xdiag[<\Xn>]+\Xpsii\sum_n Q(\Xs)<\Xx{\Xx}^T> \\
  &=\Xdiag[\frac{a}{b^s_1},...,\frac{a}{b^s_Q}]+\Xpsii\sum_n Q(\Xs)\left(\Xxm{\Xxm}^T+\Xxv\right) \label{eq:ulvppli}
\end{align}
Eq.~\ref{eq:ulvppli} follows from eq.~\ref{eq:xx}. \\
$\Xlvplui$ is $Q\times 1$:
\begin{align}
  \Xlvplui
  &=\Xpsii_{pp}\sum_n Q(\Xs)<\Xx> \\
  &=\Xpsii_{pp}\sum_n Q(\Xs)\Xxm
\end{align}
$\Xlvpuu$ is $1\times 1$:
\begin{align}
  \Xlvpuui&=
  \nu^*_p+\Xpsii_{pp}\sum_n Q(\Xs)
\end{align}

\subsection{$Q(\Xx|\Xs)$}
\begin{align}
  Q(\Xx|\Xs)=\mathcal{N}(\Xx|\Xxm,\Xxv)
\end{align}
\begin{align}
  \Xxm
  &=\Xxv<{\Xl}^T\Xpsii(\Xy-\Xu)> \\
  &=\Xxv\left[{\Xlm}^T\Xpsii\left(\Xy-\Xum\right)-\bs{v}\right] \label{eq:uxxm}
\end{align}
Eq.~\ref{eq:uxxm} follows from eq.~\ref{eq:XAy} and $\bs{v}$ is defined as
\begin{align}
  v_q
  &=\Xtr[\Xpsii\Xcov[\Xl_{:q},\Xu]] \\
  &=\sum_p \Xpsii_{pp} {\Xlvplu}_q. \label{eq:uxxmv}
\end{align}
Eq.~\ref{eq:uxxmv} follows from $\Xcov[\Xl_{pq},\mu^s_{p^\prime}]=0$ if $p\neq p^\prime$ and $\Xcov[\Xl_{pq},\mu^s_{p}]={\Xlvplu}_q$.
\begin{align}
  {\Xxv}^{-1}=<{\Xl}^T \Xpsii \Xl>+I
\end{align}
$<{\Xl}^T \Xpsii \Xl>$ is solved by eq. $\ref{eq:XAY}$ and $\Xcov[\Xl_{pi},\Xl_{p^\prime j}]=0$ for $p\ne p^\prime$:
\begin{align}
  <{\Xl}^T \Xpsii \Xl>_{ij}&=<{\Xl_{:i}}^T \Xpsii \Xl_{:j}> \\
  &=<\Xl_{:i}>^T\Xpsii <\Xl_{:j}>+\sum_p\sum_{p^\prime} \Xpsii_{pp^\prime}\Xcov[\Xl_{pi},\Xl_{p^\prime j}] \\
  &=<\Xl_{:i}>^T\Xpsii <\Xl_{:j}>+\sum_p \Xpsii_{pp}\Xcov[\Xl_{pi},\Xl_{pj}] \\
  &={\Xlm_{:i}}^T\Xpsii \Xlm_{:j}+\sum_p \Xpsii_{pp}{\Xlvpll}_{ij}
\end{align}

\subsection{$Q(\Xs)$}
\begin{align}
  \ln Q(\Xs)
  &=\psi(\alpha m_s)+\frac{1}{2}\ln |\Xxv|+<\ln P(\Xy|\Xx,\Xs,\Xl,\Psi)> \label{eq:usqe} \\
  &=\psi(\alpha m_s)+\frac{1}{2}\ln |\Xxv| \nonumber \\
  &\quad-\frac{1}{2}\Xtr[\Xpsii(\Xy-\Xltm\Xxtm)(\Xy-\Xltm\Xxtm)^T] \nonumber \\
  &\quad-\frac{1}{2}\sum_p\Xpsii_{pp}\left(\sum_q\Xltmpq{}^2\Xxtv_{qq}+\Xxtmq{}^2\Xltvp_{qq}+\Xltvp_{qq}\Xxtv_{qq}\right)
\end{align}
$\Xxt$ is defined as:
\begin{align}
  \Xxt=
  \begin{bmatrix}
    \Xx \\
    1
  \end{bmatrix}
\end{align}
The expectation in eq.~\ref{eq:usqe} is:
\begin{align}
  <\ln P(\Xy|\Xx,\Xs,\Xl,\Psi)>=-\frac{1}{2}\Xtr[\Xpsii<(\Xy-\Xlt\Xxt)(\Xy-\Xlt\Xxt)^T>]+\text{const} \label{eq:usle}
\end{align}
\begin{align}
  <(\Xy-\Xlt\Xxt)(\Xy-\Xlt\Xxt)^T>
  &=(\Xy-\Xltm\Xxtm)(\Xy-\Xltm\Xxtm)^T+\Xcov[\Xy-\Xlt\Xxt]] \label{eq:use2} \\
  &=(\Xy-\Xltm\Xxtm)(\Xy-\Xltm\Xxtm)^T+V \label{eq:use}
\end{align}
Eq.~\ref{eq:use2} follows from eq.~\ref{eq:xx}. \\
$V$ is diagonal, since rows are indepenent:
\begin{align}
  V=
  \begin{bmatrix}
    \Xvar[\Xltb_1{}^T\Xxt] & \cdots & 0 \\
    \cdots & \Xvar[\Xltb_p{}^T\Xxt] & \cdots \\
    0   & \cdots & \Xvar[\Xltb_P{}^T\Xxt]
  \end{bmatrix}
\end{align}
\begin{align}
  \Xvar[\Xltp{}^T\Xxt]
  &=\sum_q\Xvar[\Xltpq\Xxtq] \\
  &=\sum_q<\Xltpq{}^2><\Xxtq{}^2>-<\Xltpq>^2<\Xxtq>^2 \label{eq:usv1} \\
  &=\sum_q(<\Xltpq>^2+\Xvar[\Xltpq])(<\Xxtq>^2+\Xvar[\Xxtq])-<\Xltpq>^2<\Xxtq>^2 \label{eq:usv2} \\
  &=\sum_q<\Xltpq>^2\Xvar[\Xxtq]+<\Xxtq>^2\Xvar[\Xltpq]+\Xvar[\Xltpq]\Xvar[\Xxtq] \\
  &=\sum_q\Xltmpq{}^2\Xxtv_{qq}+\Xxtmq{}^2\Xltvp_{qq}+\Xltvp_{qq}\Xxtv_{qq}
\end{align}
Eq.~\ref{eq:usv1} follows from eq.~\ref{eq:vxy}, eq.~\ref{eq:usv2} from eq.~\ref{eq:x2}. \\
Eq.~\ref{eq:use} can now be resubsituted into eq.~\ref{eq:usle}:
\begin{align}
  <\ln P(\Xy|\Xx,\Xs,\Xl,\Psi)>
  &=-\frac{1}{2}\Xtr[\Xpsii <(\Xy-\Xlt\Xxt)(\Xy-\Xlt\Xxt)^T>] \\
  &=-\frac{1}{2}\Xtr[\Xpsii \left((\Xy-\Xltm\Xxtm)(\Xy-\Xltm\Xxtm)^T+V\right)] \\
  &=-\frac{1}{2}\Xtr[\Xpsii(\Xy-\Xltm\Xxtm)(\Xy-\Xltm\Xxtm)^T + \Xpsii V] \\
  &=-\frac{1}{2}\Xtr[\Xpsii(\Xy-\Xltm\Xxtm)(\Xy-\Xltm\Xxtm)^T] \nonumber \\
  &\quad-\frac{1}{2}\sum_p\Xpsii_{pp}\left(\sum_q\Xltmpq{}^2\Xxtv_{qq}+\Xxtmq{}^2\Xltvp_{qq}+\Xltvp_{qq}\Xxtv_{qq}\right) \label{eq:usef}
\end{align}
Eq.~\ref{eq:usef} can now be resubsituted into eq.~\ref{eq:usqe}:
\begin{align}
  \ln Q(\Xs)
  &=\psi(\alpha m_s)+\frac{1}{2}\ln |\Xxv|+<\ln P(\Xy|\Xx,\Xs,\Xl,\Psi)> \\
  &=\psi(\alpha m_s)+\frac{1}{2}\ln |\Xxv| \nonumber \\
  &\quad-\frac{1}{2}\Xtr[\Xpsii(\Xy-\Xltm\Xxtm)(\Xy-\Xltm\Xxtm)^T] \nonumber \\
  &\quad-\frac{1}{2}\sum_p\Xpsii_{pp}\left(\sum_q\Xltmpq{}^2\Xxtv_{qq}+\Xxtmq{}^2\Xltvp_{qq}+\Xltvp_{qq}\Xxtv_{qq}\right)
\end{align}

\section{Equations}
\newcommand{\Xvx}{\bs{x}}
\newcommand{\Xvy}{\bs{y}}

\begin{align}
  \label{eq:x2}
  <\bs{x}^2>=<\bs{x}>^2+\Xvar[\bs{x}]
\end{align}
\begin{align}
  \label{eq:xAx}
  <\bs{x}^T A \bs{x}>=<\bs{x}>^T A <\bs{x}> + \Xtr[A \Xvar[\bs{x}]]
\end{align}
\begin{align}
  \label{eq:xAy}
  <\bs{x}^T A \bs{y}>
  &=<\bs{x}>^T A <\bs{y}> + \Xtr[A \Xcov[\bs{y},\bs{x}]] \\
  &=<\bs{x}>^T A <\bs{y}> + \Xvec[A]^T\Xvec[\Xcov[\bs{x},\bs{y}]]
\end{align}
\begin{align}
  \label{eq:XAY}
  <X A Y>&=<X>A<Y>+V \\
  V_{ij}&=\Xtr[A\Xcov[Y_{:j},X_i]]=\Xvec[A]^T\Xvec[\Xcov[X_i,Y_{:j}]] \nonumber
\end{align}
\begin{align}
  \label{eq:XAy}
  <X A \bs{y}>&=<X>A<\bs{y}>+\bs{v} \\
  \bs{v}_{i}&=\Xtr[A\Xcov[\bs{y},A_i]]=\Xvec[A]^T\Xvec[\Xcov[X_i,\bs{y}]] \nonumber
\end{align}
\begin{align}
  \label{eq:xx}
  <\bs{x}\bs{x}^T>=<\bs{x}><\bs{x}>^T+ \Xcov[\bs{x}]
\end{align}
If $M$ and $\Xvx$ are independent:
\begin{align}
  \label{eq:Mx}
  <M \Xvx>=<M><\Xvx>
\end{align}
If $\Xvx$ and $\Xvy$ are independent:
\begin{align}
  \Xvar[\Xvx^T\Xvy]
  &=\sum_i \Xvar[\Xvx_i \Xvy_i] \nonumber\\
  &=\sum_i <\Xvx_i^2><\Xvy_i^2> - <\Xvx_i>^2<\Xvy_i>^2 \label{eq:vxy}
\end{align}



\end{document}
