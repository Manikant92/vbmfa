\documentclass[a4paper]{article}
\usepackage[
    bookmarks,
    bookmarksopen=true,
    backref,
    plainpages=false,
    pdfpagelabels,
    hypertexnames=false,
    linktocpage,
    colorlinks=true,
    linkcolor=blue,
    anchorcolor=blue,
    citecolor=blue,
    filecolor=blue,
    menucolor=blue,
    urlcolor=cyan, 
]{hyperref}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{booktabs}
\usepackage{hyperref}


\author{Angermueller Christof}
\date{20 January 2014}
\title{Variational Inference for Bayesian Mixture of Factor Analysers}

\begin{document}
\maketitle

\newcommand{\bs}{\boldsymbol}
\newcommand{\Xp}{\bs{\pi}}
\newcommand{\Xbl}{\bar{\Lambda^s}}
\newcommand{\Xn}{\bs{\nu}^s}
\newcommand{\Xe}{\Psi}
\newcommand{\Xa}{\alpha}
\newcommand{\Xc}{\Xl{\Xl}^T}
\newcommand{\Xl}{\Lambda^s}
\newcommand{\Xlp}{\overline{\Lambda}^s}
\newcommand{\Xsp}{\Sigma^{p,s}}
\newcommand{\Xx}{\bs{x}^n}
\newcommand{\Xxm}{\overline{\bs{x}}^{n,s}}
\newcommand{\Xxs}{\Sigma^s}
\newcommand{\Xlm}{\overline{\Lambda}^s}
\newcommand{\Xls}{\Sigma^{p,s}}
\newcommand{\Xpsii}{\Psi^{-1}}
\newcommand{\Xy}{\bs{y}^n}
\newcommand{\Xs}{s^n}
\newcommand{\Xcov}{\operatorname{cov}}
\newcommand{\Xvar}{\operatorname{var}}
\newcommand{\Xtr}{\operatorname{tr}}
\newcommand{\Xdiag}{\operatorname{diag}}

\section{Update equations}
\begin{align}
  Q(\bs{x},\bs{s},\bs{\pi},\bs{\Lambda},\bs{\nu})=Q(\Xp)\prod_{s,p}Q(\Xl_p)\prod_{s,q}Q(\nu_q^s)\prod_n Q(\Xx|\Xs)Q(\Xs)
\end{align}

\subsection{$Q(\Xp)$}
\begin{align}
  Q(\Xp)\sim\mathcal{D}(\omega\bs{u})
\end{align}

\begin{align}
  \omega u_s=\frac{\alpha}{S}+\sum_n Q(s^n)
\end{align}

\subsection{$Q(\Xl_p)$}
\begin{align}
  Q(\Xl_p)\sim\mathcal{N}(\Xlm_p,\Xls)
\end{align}

\begin{align}
  \Xlm=\left[\Xpsii\sum_n Q(s^n)\Xy{\Xxm}^T\Xls\right]
\end{align}

\begin{align}
  {\Xls}^{-1}&=\Psi^{-1}_{pp}\sum_n Q(s^n)<\Xx {\Xx}^T>+\Xdiag[<\Xn>] \\
    &=\Psi^{-1}\sum_n Q(s^n)\left(\Xxm{\Xxm}^T+\Xxs\right)+\Xdiag[\frac{a^s_1}{b^s_1},\dots,\frac{a^s_Q}{b^s_Q}]
\end{align}
Here, we solved $<\Xx {\Xx}^T>$ by eq. \ref{eq:xx}.
 
\subsection{$Q(\nu_q^s)$}
\begin{align}
 Q(\nu_q^s)\sim\mathcal{G}(a_q^s, b_q^s)
\end{align}

\begin{align}
a_q^s&=a+\frac{P}{2} \\
b_q^s&=b+\frac{1}{2}\sum_p<{\Lambda^s_{pq}}^2> \\
&=b+\frac{1}{2}\sum_p {\overline{\Lambda}_{pq}^s}^2+\Sigma_{qq}^{s,p}
\end{align}
Here, we solved $<{\Lambda^s_{pq}}^2>$ by eq. \ref{eq:x2}.

\subsection{$Q(\Xx|\Xs)$}
\begin{align}
  Q(\Xx|\Xs)\sim\mathcal{N}(\Xxm,\Xxs)
\end{align}

\begin{align}
  \Xxm=\Xxs{\Xlm}^T\Xpsii\Xy
\end{align}

\begin{align}
  {\Xxs}^{-1}=<{\Xl}^T \Xpsii \Xl>+I
\end{align}
We solve $<{\Xl}^T \Xpsii \Xl>$ by eq. $\ref{eq:xAy}$ and using that $\Xcov[\Xl_p,\Xl_{p^\prime}]=0$ for $p\ne p^\prime$:
\begin{align}
  <{\Xl}^T \Xpsii \Xl>_{ij}&=<{\Xl_{:i}}^T \Xpsii \Xl_{:j}> \\
  &=<\Xl_{:i}>^T\Xpsii <\Xl_{:j}>+\sum_p\sum_{p^\prime} \Xpsii_{pp^\prime}\Xcov[\Xl_{pi},\Xl_{p^\prime j}] \\
  &=<\Xl_{:i}>^T\Xpsii <\Xl_{:j}>+\sum_p \Xpsii_{pp}\Xcov[\Xl_{pi},\Xl_{pj}] \\
  &={\Xlm_{:i}}^T\Xpsii \Xlm_{:j}+\sum_p \Xpsii_{pp}\Xls_{ij}
\end{align}

\subsection{$Q(\Xs)$}
\begin{align}
  \ln Q(\Xs)
  &=\left[\psi(\omega u_s)-\psi(\omega)\right]+\frac{1}{2}\ln |\Xxs|+<\ln P(\Xy|\Xx,\Xs,\Xl,\Psi)>+\text{const} \\
  &\propto \psi(\omega u_s)+\frac{1}{2}\ln|\Xxs|-\frac{1}{2}(\Xy-\Xlm\Xxm)^T\Xpsii(\Xy-\Xlm\Xxm) \\
  &\qquad-\frac{1}{2}\sum_p \Xpsii_{pp}\Xdiag[\Xls]^T\Xdiag[\Xxs]
\end{align}

Here we solved for $<\ln P(\Xy|\Xx,\Xs,\Xl,\Psi)>$ and ignored constant terms:
\begin{align}
<\ln P(\Xy|\Xx,\Xs,\Xl,\Psi)>=-\frac{P}{2}\ln 2\pi-\frac{1}{2}\ln|\Xpsii|-\frac{1}{2}<(\Xy-\Xl\Xx)^T\Xpsii(\Xy-\Xl\Xx)>
\end{align}

We solve $<(\Xy-\Xl\Xx)^T\Xpsii(\Xy-\Xl\Xx)>$ by eq. \ref{eq:xAx}:
\begin{align}
  <(\Xy&-\Xl\Xx)^T\Xpsii(\Xy-\Xl\Xx)> \\
  &=<\Xy-\Xl>^T\Xpsii<\Xy-\Xl\Xx>+\Xtr[\Xpsii V] \\
  &=(\Xy-<\Xl><\Xx>)^T\Xpsii(\Xy-<\Xl><\Xx>)+\Xtr[\Xpsii V] \\
  &=(\Xy-\Xlm\Xxm)^T\Xpsii(\Xy-\Xlm\Xxm)+\Xtr[\Xpsii V]
\end{align}

The covariance matrix $V$ is diagonal since $\Xcov[\Xl_p,\Xl_{p^\prime}]=0$ for $p\ne p^\prime$:
\begin{align}
  V:=\Xvar[\Xl\Xx]=
  \begin{bmatrix}
  \Xvar[\Xl_1\Xx] & \cdots & 0 \\
  \cdots & \Xvar[\Xl_p\Xx] & \cdots \\
  0   & \cdots & \Xvar[\Xl_P\Xx]
  \end{bmatrix}
\end{align}
To solve $\Xvar[\Xl_p\Xx]$, we use eq. \ref{eq:vxy} and \ref{eq:x2}, and assume $<\Xl_{pq}>=0$, $<\Xx_q>=0$:
\begin{align}
  \Xvar[\Xl_p\Xx]
  &=\sum_q<{\Xl_{pq}}^2><{\Xx_q}^2>-<{\Xl_{pq}}>^2<{\Xx_q}>^2 \\
  &=\sum_q<{\Xl_{pq}}^2><{\Xx_q}^2> \\
  &=\sum_q (<{\Xl_{pq}}>^2+\Xvar[\Xl_{pq}])(<{\Xx_{q}}>^2+\Xvar[{\Xx_{q}}]) \\
  &=\sum_q \Xvar[\Xl_{pq}]\Xvar[{\Xx_{q}}] \\
  &=\sum_q \Xls_{qq}\Xxs_{qq}=\Xdiag[\Xls]^T\Xdiag[\Xxs]
\end{align}
The trace is thus
\begin{align}
  \Xtr[\Xpsii V]=\sum_p \Xpsii_{pp}\Xdiag[\Xls]^T\Xdiag[\Xls]
\end{align}
which results in
\begin{align}
  <(\Xy&-\Xl\Xx)^T\Xpsii(\Xy-\Xl\Xx)> \\
  &=(\Xy-\Xlm\Xxm)^T\Xpsii(\Xy-\Xlm\Xxm)+\sum_p \Xpsii_{pp}\Xdiag[\Xls]^T\Xdiag[\Xls].
\end{align}

\section{Equations}
\newcommand{\Xvx}{\bs{x}}
\newcommand{\Xvy}{\bs{y}}

\begin{align}
  \label{eq:x2}
  <\bs{x}^2>=<\bs{x}>^2+\Xvar[\bs{x}]
\end{align}

\begin{align}
  \label{eq:xAx}
  <\bs{x}^T A \bs{x}>=<\bs{x}>^T A <\bs{x}> + \Xtr[A \Xvar[\bs{x}]]
\end{align}

\begin{align}
  \label{eq:xAy}
  <\bs{x}^T A \bs{y}>
  &=<\bs{x}>^T A <\bs{y}> + \Xtr[A \Xcov[\bs{y},\bs{x}]] \\
  &=<\bs{x}>^T A <\bs{y}> + \sum_i\sum_j A_{ij} \Xcov[\bs{x}_i,\bs{y}_j]
\end{align}

\begin{align}
  \label{eq:xx}
  <\bs{x}\bs{x}^T>=<\bs{x}><\bs{x}>^T+ \Xcov[\bs{x}]
\end{align}

If $M$ and $\Xvx$ are independent:
\begin{align}
  \label{eq:Mx}
  <M \Xvx>=<M><\Xvx>
\end{align}

If $\Xvx$ and $\Xvy$ are independent:
\begin{align}
  \label{eq:vxy}
  \Xvar[\Xvx^T\Xvy]
  &=\sum_i \Xvar[\Xvx_i \Xvy_i] \\
  &=\sum_i <\Xvx_i^2><\Xvy_i^2> - <\Xvx_i>^2<\Xvy_i>^2 
\end{align}

\end{document}
